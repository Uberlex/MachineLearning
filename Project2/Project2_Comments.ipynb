{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "necessary-chair",
   "metadata": {},
   "source": [
    "# **Import the Necessary Tools**\n",
    "---\n",
    "The primary tools to consolidate the neural network, process the data, and pool the resorts all come from a python-coded library known as TensorFlow. TensorFlow is utilized for a variety of purpose from sequence-to-sequence models, word embeddings, and handwritten symbol classification. This project utilizes Tensorflow's tools of image recognition and processing; taking visual data imported from human programmers and running them through the neural net to identify specific entities and objects within the imagery. \n",
    "\n",
    "The process to carry out this project is implemented by a Tensorflow subsidary known as Keras. Keras is an API used to make Tensorflow's tools not only further compatible with the Python coding, but simplifies tool functions for human users, making it's interface easier to understand. \n",
    "\n",
    "Visualization of processing the data and measuring the neural nets educational strength is conducted with matplotlib, allowing programmers to create a wide variety of graphs based off of functional inputs, in this case from the neural net itself. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "royal-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import PIL\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Pillow version:\",PIL.__version__,\"\\n\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "automatic-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestMap = pd.read_csv(\"./testsam.csv\") # Import the data to feed the neural network with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rubber-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestMap                                # Display the data for success verification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-flight",
   "metadata": {},
   "source": [
    "---\n",
    "## **Operating the Neural Network** \n",
    "---\n",
    "The neural network in in question is made up of several layers, convolutional and pooling in nature, followed by fully connected layers, the convolutional layers are used to extract features from the input images while the pooling layers reduce the spatial dimensions of the feature maps. The feature maps are the organization of the 'TestMap' data to be processed in a specific order or structure.  The fully connected layers use the features to classify the images into one of six possible categories. The model is optimized using the categorical cross-entropy loss function and the Adam optimizer, with accuracy as the evaluation metric.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naked-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3),activation='relu', input_shape=(500,500, 3)))\n",
    "model.add(Conv2D(64, kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3),activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3),activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "'''\n",
    "model.add(Conv2D(256, kernel_size=(3,3),activation='relu', use_bias=False))\n",
    "model.add(Conv2D(256, kernel_size=(3,3),activation='relu', use_bias=False))\n",
    "model.add(Conv2D(256, kernel_size=(3,3),activation='relu', use_bias=False))\n",
    "model.add(Conv2D(256, kernel_size=(3,3),activation='relu', use_bias=False))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3),activation='relu', use_bias=False))\n",
    "model.add(Conv2D(512, kernel_size=(3,3),activation='relu', use_bias=False))\n",
    "model.add(Conv2D(512, kernel_size=(3,3),activation='relu', use_bias=False))\n",
    "model.add(Conv2D(512, kernel_size=(3,3),activation='relu', use_bias=False))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "'''\n",
    "\n",
    "model.add(Flatten(input_shape=(10,10,1)))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "          \n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-adventure",
   "metadata": {},
   "source": [
    "---\n",
    "## **Display all Findings**\n",
    "---\n",
    "The layout of the model's findings are divided into three different categories. The first of which is the layer within the neural network that the data is fed through for processing. Each layer interprets the data divergent from it's predecessor. The \"Output Shape\" column is the aftermath of the data through the respective layer in the form of matrices. The first placeholder within the parenthesis, all of them labeled 'None' are the size of the batch of data fed through the layer. In the case of this neural net, the data was pushed through the network all at once, not in small increments. Therefore, the neural net had no reason to measure it's size. The numbers within the parenthesis represent how big each dimension of the data is when fed through their respective layers. For example, (498,498,64) means the data has a height and width of 498 pixels and a total of 64 filters applied to each photo in the data within the layer.  The category of parameters represents how many variables within each layer the neural network educates itself with in order to finalize it's conclusions for the data. Every number in each row of the \"Param #\" column is determined by how the layers are organized and in what way the data is given to them. In the case of this Convolutional Neural Network or (CNN), parameters come from the amount of filters dimensions, inputs, and outputs in addition to potential bias within the data. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absent-rates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 498, 498, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 496, 496, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 248, 248, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 246, 246, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 244, 244, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 122, 122, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 120, 120, 256)     295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 118, 118, 256)     590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 59, 59, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 891136)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               89113700  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 90,269,814\n",
      "Trainable params: 90,269,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # Display the final result of the neural nets findings, all parameters are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incoming-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weights from the callback file so the neural network can be trained from previous data processing sessions\n",
    "model.load_weights(\"Files/Week_14/tmp_1200_runs/callbacks\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-aaron",
   "metadata": {},
   "source": [
    "---\n",
    "## **Building the test data function**\n",
    "---\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "linear-difficulty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_df = pd.read_csv('testsam.csv')\n",
    "test_dir = \"/home/Shared/CAMS\"\n",
    "target_size = (500, 500)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "        test_df,\n",
    "        directory=test_dir,\n",
    "        x_col='id',\n",
    "        y_col=None,\n",
    "        target_size=target_size,\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "underlying-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = {'bird': 0, 'coyote': 1, 'deer': 2, 'human': 3, 'rabbit': 4, 'squirrel': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intellectual-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reasonable-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in a:\n",
    "    output_array = np.array(i)\n",
    "    max_index = output_array.argmax()\n",
    "    result.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "equipped-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = {'bird': 0, 'coyote': 1, 'deer': 2, 'human': 3, 'rabbit': 4, 'squirrel': 5}\n",
    "\n",
    "name_list = []\n",
    "for name in result:\n",
    "    a = list(classify.keys())[list(classify.values()).index(name)]\n",
    "    name_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "democratic-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = np.array(TestMap[\"POA_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "political-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(label_list)):\n",
    "    if label_list[i] == name_list[i]:\n",
    "        correct +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "disabled-shelf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['human', 'squirrel', 'rabbit', 'rabbit', 'human', 'rabbit',\n",
       "       'human', 'human', 'squirrel', 'deer', 'deer', 'coyote', 'deer',\n",
       "       'squirrel', 'human', 'squirrel', 'squirrel', 'squirrel',\n",
       "       'squirrel', 'deer', 'bird', 'coyote', 'deer', 'bird', 'coyote',\n",
       "       'squirrel', 'bird', 'rabbit', 'coyote', 'deer', 'deer', 'human',\n",
       "       'squirrel', 'squirrel', 'rabbit', 'squirrel', 'coyote', 'deer',\n",
       "       'squirrel', 'rabbit', 'coyote', 'deer', 'deer', 'bird', 'squirrel',\n",
       "       'squirrel', 'deer', 'human', 'deer', 'deer', 'rabbit', 'deer',\n",
       "       'deer', 'squirrel', 'rabbit', 'coyote', 'squirrel', 'deer',\n",
       "       'squirrel', 'coyote', 'squirrel', 'squirrel', 'coyote', 'squirrel',\n",
       "       'deer', 'human', 'deer', 'coyote', 'coyote', 'human', 'deer',\n",
       "       'deer', 'deer', 'coyote', 'coyote', 'rabbit', 'rabbit', 'squirrel',\n",
       "       'bird'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ahead-validity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human',\n",
       " 'rabbit',\n",
       " 'deer',\n",
       " 'rabbit',\n",
       " 'human',\n",
       " 'deer',\n",
       " 'human',\n",
       " 'human',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'rabbit',\n",
       " 'human',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'human',\n",
       " 'squirrel',\n",
       " 'deer',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'deer',\n",
       " 'human',\n",
       " 'rabbit',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'rabbit',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'rabbit',\n",
       " 'rabbit',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'squirrel',\n",
       " 'rabbit',\n",
       " 'squirrel',\n",
       " 'deer',\n",
       " 'human',\n",
       " 'squirrel',\n",
       " 'rabbit',\n",
       " 'squirrel',\n",
       " 'deer',\n",
       " 'human',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'rabbit',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'squirrel',\n",
       " 'rabbit',\n",
       " 'squirrel',\n",
       " 'human',\n",
       " 'deer',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'rabbit',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'deer',\n",
       " 'human',\n",
       " 'deer',\n",
       " 'squirrel',\n",
       " 'rabbit',\n",
       " 'human',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'deer',\n",
       " 'rabbit',\n",
       " 'squirrel',\n",
       " 'squirrel',\n",
       " 'rabbit',\n",
       " 'rabbit']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threatened-friend",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1bff7643b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclass_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "class_indices = train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-saying",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
