{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lecture: Logistic Regression using Gradient Descent** (Liver Data Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/uciml/indian-liver-patient-records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Numpy\n",
    "import pandas as pd # Pandas is a great tool for working with and displaying data.\n",
    "import matplotlib.pyplot as plt # For plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>Sex</th>\n",
       "      <th>TOTAL_BIL</th>\n",
       "      <th>DIR_BIL</th>\n",
       "      <th>ALK_PHO</th>\n",
       "      <th>ALA_AMINO</th>\n",
       "      <th>ASP_AMI</th>\n",
       "      <th>TOTAL_PROT</th>\n",
       "      <th>ALBU</th>\n",
       "      <th>ALBU_GLO</th>\n",
       "      <th>DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AGE  Sex  TOTAL_BIL  DIR_BIL  ALK_PHO  ALA_AMINO  ASP_AMI  TOTAL_PROT  \\\n",
       "1  65.0  1.0        0.7      0.1    187.0       16.0     18.0         6.8   \n",
       "2  62.0  2.0       10.9      5.5    699.0       64.0    100.0         7.5   \n",
       "3  62.0  2.0        7.3      4.1    490.0       60.0     68.0         7.0   \n",
       "4  58.0  2.0        1.0      0.4    182.0       14.0     20.0         6.8   \n",
       "5  72.0  2.0        3.9      2.0    195.0       27.0     59.0         7.3   \n",
       "\n",
       "   ALBU  ALBU_GLO  DATA  \n",
       "1   3.3      0.90   1.0  \n",
       "2   3.2      0.74   1.0  \n",
       "3   3.3      0.89   1.0  \n",
       "4   3.4      1.00   1.0  \n",
       "5   2.4      0.40   1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Prep Step for liver set.\n",
    "#RAW_DATA = np.genfromtxt(\"~/Data/healthcare/liver\", delimiter=',')\n",
    "RAW_DATA_FRAME = pd.read_csv(\"~/Files/Data/Liver/indian_liver_patient.csv\", header = None)\n",
    "RAW_DATA_FRAME = RAW_DATA_FRAME.iloc[1:] # Remove the 0 row.\n",
    "\n",
    "# Give the columns keywords.\n",
    "Colnames = ['AGE', 'Sex', 'TOTAL_BIL', 'DIR_BIL', 'ALK_PHO', 'ALA_AMINO', 'ASP_AMI', 'TOTAL_PROT', 'ALBU', 'ALBU_GLO', 'DATA']\n",
    "RAW_DATA_FRAME.columns = Colnames \n",
    "\n",
    "# Make an index for the included genders.\n",
    "mapping = {'Female': 1, 'Male': 2}\n",
    "\n",
    "# Map Female to 1 and Male to 2.\n",
    "RAW_DATA_FRAME[\"Sex\"] = [mapping[item] for item in RAW_DATA_FRAME[\"Sex\"]]\n",
    "\n",
    "# Make all the entries in the dataframe into floats. (They were read in as strings.)\n",
    "cols = RAW_DATA_FRAME.columns\n",
    "for col in cols:\n",
    "    RAW_DATA_FRAME[col] = RAW_DATA_FRAME[col].astype(float)\n",
    "\n",
    "# Print a small part of the data.\n",
    "RAW_DATA_FRAME.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2.]), array([416, 167]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(RAW_DATA_FRAME[\"DATA\"], return_counts= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block is for when we switch the the Iris set.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Working to make training and testing data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section of code allows us to plot different features with respect to each other. For this method of machine learning we are looking for a linear correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA_LABELS = [\"ALBU\", \"TOTAL_BIL\", \"DIR_BIL\", \"ALA_AMINO\",\"AGE\",\"ALK_PHO\",\"ASP_AMI\", \"DATA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA_SHUFFLED = RAW_DATA_FRAME[ALL_DATA_LABELS].sample(frac=1) # Shuffle the data frame\n",
    "TRAINING = ALL_DATA_SHUFFLED[:400] # Build the training.\n",
    "TESTING = ALL_DATA_SHUFFLED[400:] # Build the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALBU</th>\n",
       "      <th>TOTAL_BIL</th>\n",
       "      <th>DIR_BIL</th>\n",
       "      <th>ALA_AMINO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>ALK_PHO</th>\n",
       "      <th>ASP_AMI</th>\n",
       "      <th>DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>3.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ALBU  TOTAL_BIL  DIR_BIL  ALA_AMINO   AGE  ALK_PHO  ASP_AMI  DATA\n",
       "16    2.3        0.6      0.1       91.0  25.0    183.0     53.0   2.0\n",
       "250   3.0        1.2      0.3       28.0  33.0    498.0     25.0   1.0\n",
       "350   3.6        0.8      0.2       19.0  49.0    158.0     15.0   2.0\n",
       "74    3.6        0.6      0.1       22.0  52.0    171.0     16.0   1.0\n",
       "446   4.0        0.9      0.2       40.0  17.0    279.0     46.0   2.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN =  np.array(TRAINING.drop(\"DATA\",  axis=1)) # Remove the target column\n",
    "Y_TRAIN = np.array(TRAINING[\"DATA\"] - 1) # Change the targets to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST = np.array(TESTING.drop(\"DATA\",  axis=1))\n",
    "Y_TEST = np.array(TESTING[\"DATA\"] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression with Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Goal: Find a line that best fits the data.**\n",
    "\n",
    "---\n",
    "First recall that the equation of the sigmoid is:\n",
    "$$ \\textrm{Sigmoid function:} \\ \\ S(x) = \\frac{1}{1 + e^{-x}} $$  \n",
    "$$ \\textrm{Sigmoid function's Derivative:} \\ \\ S'(x) = S(x)(1 - S(x)) $$  \n",
    "\n",
    "Now, each data value will be written as,\n",
    "$$z = b + w_1 x_1 + w_2 x_2 + ... + w_m x_m = b +  \\mathbf{W}^{T} \\cdot \\mathbf{X}   $$  \n",
    "If we take this value, and evaluate it in the sigmoid, we will get a value between 0 and 1. So,\n",
    "$$S(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def S(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "t = np.arange(-10,10, .01)\n",
    "plt.plot(t,S(t))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are going to use gradient decent to find the best values for $b ,w_1,w_2...w_m$.  \n",
    "There is one more thing that we will need. Since the idea here is to output information for a binary situation, we need to make sure that our target data is 0 or 1. We did this above in the data prep step.\n",
    "$$\\hat{y} = \\begin{cases} \n",
    "      0 & y_i \\ \\ \\textbf{is positive} \\\\\n",
    "      1 & y_i \\ \\ \\textbf{is negative}\n",
    "   \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function that we want to minimize is:\n",
    "$$ CE(b,w_1,w_2...w_m) = -\\frac{1}{N}\\sum_{i=1}^{N}\\hat{y}_{i} \\log(S(z)) + (1 - \\hat{y}_{i}) \\log(1 - S(z)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Compute the Gradient:**  \n",
    "The following is the notation that we will need to write down this important function. Since everything that we will be working with from here on out will have multiple feature vectors, we are going to be using gradient as the derivative we will be considering.\n",
    "\n",
    "So, let's compute this object:  \n",
    "$$\\nabla CE(b,w_1,w_2...w_m) = \\left\\langle \\frac{1}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , \\frac{x_1}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , \\frac{x_2}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , ... , \\frac{x_m}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) \\right\\rangle $$  \n",
    "\n",
    "So to apply gradient descent we need to evaluate this vector over and over as we update the weights,\n",
    "$$ \\langle b, w_1, w_2, w_3, ... ,w_m \\rangle - \\left\\langle \\frac{1}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , \\frac{x_1}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , \\frac{x_2}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , ... , \\frac{x_m}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) \\right\\rangle $$  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S(z) = \\frac{1}{1 + e^{-z}} $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makegradient(X, Y, weights):\n",
    "    Y = np.array(Y) # Make the target a numpy array.\n",
    "    N  = len(X) # Assign the working length.\n",
    "    \n",
    "    # Make the linear z values.\n",
    "    z = (weights *  X).sum(axis=1)    \n",
    "     \n",
    "    # Apply the sigmoid.\n",
    "    Squshed_Z = sigmoid(z)\n",
    "    \n",
    "    # Make the gradients.\n",
    "    B = np.dot(np.ones((len(Y),1)).T, Squshed_Z - Y) / N\n",
    "    S = np.dot(X.T, Squshed_Z - Y) / N\n",
    "    \n",
    "    return [S , B] # Return the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentLogisticRegression(x, Y, learning_Rate, epochs):\n",
    "        \n",
    "    total_data = len(Y) # This is the N from above.\n",
    "    weights = np.zeros(len(x.T)) # Random weights for our slope and intercept.\n",
    "    B_VAL = np.zeros(1)\n",
    "    \n",
    "    # Loop over all the epochs.\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Find the gradients.\n",
    "        grad = makegradient(x, Y, weights)\n",
    "        \n",
    "        # Update the weights.\n",
    "        weights = weights - learning_Rate * grad[0]\n",
    "        print(grad[0])\n",
    "        B_VAL = B_VAL - learning_Rate * grad[1]\n",
    "\n",
    "    \n",
    "    # Return the weights, the list of intercepts and the list of slopes.\n",
    "    return [weights, B_VAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.595625  1.1285    0.569375 34.80375  10.01625  81.99625  47.36375 ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  2.01229793   2.22444798   1.07988545  39.92437641  29.99006258\n",
      " 213.85541804  52.67541847]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  1.73252561   1.56231128   0.76152043  25.81163417  26.3016082\n",
      " 188.88727878  31.55189314]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  1.86275      1.84875      0.909       29.2949999   27.91249996\n",
      " 198.34999978  37.87999983]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  1.80104761   1.68402267   0.8204358   27.63377454  27.1227932\n",
      " 193.41329986  34.24929547]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  1.86936242   1.85320676   0.91098821  29.5041194   28.04100625\n",
      " 198.58471676  38.0691036 ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  1.80849435   1.69948281   0.82923997  27.88483093  27.22243748\n",
      " 193.76470396  34.44983084]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  1.83776477   1.82260872   0.89805062  28.66326488  27.71114688\n",
      " 195.90261085  36.13073267]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  1.83269866   1.78238133   0.87687725  28.58267049  27.60429974\n",
      " 195.41948249  35.68414751]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  1.83898508   1.83233723   0.90315445  28.68194569  27.73726539\n",
      " 196.01844725  36.23893072]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[  1.82603185   1.70625225   0.8316325   28.31049686  27.50567522\n",
      " 194.71553211  35.22531553]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.9667715   -0.35244219  -0.12508238  -9.17164476 -11.99960529\n",
      " -64.89253869 -11.32329222]\n",
      "[  1.83662539   1.8054119    0.88715545  28.5857568   27.70232065\n",
      " 195.76934716  36.07641604]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.93492949  -0.3410844   -0.12152756  -8.97329123 -11.32328278\n",
      " -63.4876302  -11.09346411]\n",
      "[  1.8164837    1.6634838    0.81014638  28.04389102  27.45028397\n",
      " 194.14294386  34.94347333]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.8924575   -0.33086767  -0.11795332  -8.77566756 -10.55289606\n",
      " -61.54419172 -10.86830431]\n",
      "[  1.79998916   1.65573885   0.8072449   27.5422833   27.26237253\n",
      " 192.91655034  34.16686264]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.85466488  -0.31090615  -0.11120602  -8.47699386  -9.73773908\n",
      " -59.80418044 -10.51910784]\n",
      "[  1.78399073   1.64467703   0.80265236  27.17418018  27.10874325\n",
      " 191.40870165  33.2813923 ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.83717756  -0.29732944  -0.10510981  -8.3657359   -9.32495299\n",
      " -58.73772118 -10.36628496]\n",
      "[  1.74050241   1.57676341   0.7710062   25.48008676  26.60753354\n",
      " 186.50032807  30.92016768]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.71674991  -0.25999998  -0.09075     -7.55999923  -6.89249836\n",
      " -51.79999511  -9.44999939]\n",
      "[  1.71262092   1.5146555    0.74218603  24.0094197   26.14402754\n",
      " 181.84860927  28.86915053]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.64456359  -0.23232223  -0.08127178  -6.90871349  -5.30471904\n",
      " -47.20485264  -8.66892693]\n",
      "[  1.68125453   1.47151095   0.70500628  22.66013725  25.82755986\n",
      " 177.25271761  27.32013434]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.55893562  -0.20199458  -0.07078583  -6.17795732  -3.93313026\n",
      " -42.97360999  -8.0487149 ]\n",
      "[  1.68127194   1.47145965   0.70500876  22.66114232  25.82771722\n",
      " 177.24765987  27.31849575]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.64084281  -0.22898589  -0.08031737  -6.76491132  -5.15532824\n",
      " -46.88215999  -8.58857151]\n",
      "[  1.7054997    1.49649982   0.71749996  23.57999079  26.12999671\n",
      " 178.93996358  28.07998475]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.63642573  -0.22397863  -0.07823258  -6.66791443  -4.98108199\n",
      " -46.42283772  -8.47577319]\n",
      "[  1.70549828   1.49649898   0.71749975  23.57994656  26.12998092\n",
      " 178.93978879  28.07991159]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.63551762  -0.22524661  -0.07863801  -6.64183922  -5.08166117\n",
      " -46.96431545  -8.41489558]\n",
      "[  1.72565939   1.54871222   0.74394088  23.93534981  26.4366903\n",
      " 180.38519165  29.29915536]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.66759193  -0.24503492  -0.08611817  -7.09131132  -5.8946288\n",
      " -49.18604649  -8.88767805]\n",
      "[  1.74750005   1.57275      0.75425     24.35000248  26.63500052\n",
      " 182.02250105  30.21500075]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.65801073  -0.24354544  -0.08576026  -7.00972753  -5.76137344\n",
      " -48.90647695  -8.75084384]\n",
      "[  1.72299189   1.5479948    0.74399869  23.63725459  26.45240937\n",
      " 178.89143493  29.21706193]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.60444445  -0.21398372  -0.07399748  -6.24189106  -4.65621338\n",
      " -46.02103764  -8.0843936 ]\n",
      "[  1.72027054   1.55353701   0.74746645  23.65065734  26.48231662\n",
      " 178.31249183  29.21278889]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.97575  -0.35575  -0.1265   -9.2     -12.165   -65.2375  -11.38   ]\n",
      "[ -0.60477572  -0.21417205  -0.07408815  -6.2441694   -4.66011636\n",
      " -46.03231039  -8.08639317]\n",
      "[  1.71935339   1.55486629   0.74833489  23.93500378  26.56513792\n",
      " 177.88342745  29.32390463]\n",
      "The weights are:  [  2.33060137  -1.78077174  -1.19492179  -4.80491983  13.70670914\n",
      " -17.34969924  -6.47956747] with a b value of:  [0.57481288]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "w, b = gradientDescentLogisticRegression(X_TRAIN, Y_TRAIN, .1, 100) # Call the definition with our data sets.\n",
    "print(\"The weights are: \",  w, \"with a b value of: \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number correct:  133\n",
      "Total number in the testing data:  183\n",
      "Accuracy Score 0.726775956284153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "V = [] # Empty list to collect the predictions.\n",
    "\n",
    "x = np.array(X_TEST) # Assign the testing data.\n",
    "y = np.array(Y_TEST) # Assign the testing targets.\n",
    "\n",
    "# Loop over all the testing features to get the classification probabilities.\n",
    "for i in range(len(x)):\n",
    "    V.append(sigmoid(np.dot(w, x[i]) + b))\n",
    "\n",
    "new_y_pred = [] # Empty list for the predictions.\n",
    "\n",
    "# Set the tolerance to 50%.\n",
    "for val in V:\n",
    "    if(val >= 0.5):\n",
    "        new_y_pred.append(1)\n",
    "    else:\n",
    "        new_y_pred.append(0)    \n",
    "\n",
    "# Print the findings. (I use sklearn to help with counting here.)\n",
    "print(\"Total number correct: \", accuracy_score(y,new_y_pred, normalize = False))\n",
    "print(\"Total number in the testing data: \", len(Y_TEST))\n",
    "\n",
    "print(\"Accuracy Score\", accuracy_score(y,new_y_pred, normalize = False) / len(Y_TEST) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WARNING SKLEARN IS AWESOME!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create logistic regression object\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_skit = clf.predict(X_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73224043715847\n"
     ]
    }
   ],
   "source": [
    "new_y_pred_skit = []\n",
    "\n",
    "for val in y_pred_skit:\n",
    "    if(val >= 0.5):\n",
    "        new_y_pred_skit.append(1)\n",
    "    else:\n",
    "        new_y_pred_skit.append(0) \n",
    "\n",
    "print(accuracy_score(y,new_y_pred_skit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
